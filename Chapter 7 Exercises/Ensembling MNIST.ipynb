{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the MNIST data (This takes forever, so do it just once)\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting training, validation, and testing data\n",
    "import numpy as np\n",
    "mnist_data = mnist['data']\n",
    "mnist_test = mnist['target']\n",
    "\n",
    "# Uncomment after loading the numpy pdframe into a numpy array\n",
    "mnist_data = mnist_data.to_numpy()\n",
    "\n",
    "# np.uint8 astype will turn all the ELEMENTS into unsigned integers\n",
    "mnist_test = mnist_test.to_numpy().astype(np.uint8)\n",
    "mnist_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting up the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data = mnist_data[:60000]\n",
    "train_labels = mnist_test[:60000]\n",
    "\n",
    "# Do not use the data for the testing\n",
    "test_data = mnist_data[60000:]\n",
    "test_labels = mnist_test[60000:]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_data, train_labels, test_size=1/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9715, 0.9649, 0.9738]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf', RandomForestClassifier()),\n",
       "                             ('svc',\n",
       "                              Pipeline(steps=[('standardscaler',\n",
       "                                               StandardScaler()),\n",
       "                                              ('svc', SVC(gamma='auto'))])),\n",
       "                             ('et', ExtraTreesClassifier())],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training all the classifiers (RFC, SVC, ETC)\n",
    "# This takes SUPER LONG so only do it once!\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "svc = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "etc = ExtraTreesClassifier()\n",
    "\n",
    "myClassifiers = [rfc, svc, etc]\n",
    "\n",
    "# Train all three\n",
    "for classifiers in myClassifiers:\n",
    "    classifiers.fit(x_train, y_train)\n",
    "    \n",
    "accuracies = [estimator.score(x_val, y_val) for estimator in myClassifiers]\n",
    "print(accuracies)\n",
    "\n",
    "# Creating a voting ensemble classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('rf', rfc), ('svc', svc), ('et', etc)],\n",
    "    voting='hard')\n",
    "\n",
    "# Training the voting classifier\n",
    "voting_clf.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9748"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing the ensemble of the voting classifier\n",
    "# Messed up on the voting classifier, so I am changing the voting method\n",
    "# Creating a voting ensemble classifier\n",
    "# voting_clf = VotingClassifier(\n",
    "#   estimators=[('rf', rfc), ('svc', svc), ('et', etc)],\n",
    "#   voting='hard')\n",
    "# voting_clf.fit(x_train, y_train)\n",
    "voting_clf.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.004733333333333478\n"
     ]
    }
   ],
   "source": [
    "# The Voter did just about .5 percent better\n",
    "print((sum(accuracies)/len(accuracies)) - voting_clf.score(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9688, 0.9648, 0.9702]\n",
      "0.9721\n"
     ]
    }
   ],
   "source": [
    "# Now let's try the test set\n",
    "test_scores =[classifier.score(test_data, test_labels) for classifier in myClassifiers]\n",
    "print(test_scores)\n",
    "\n",
    "print(voting_clf.score(test_data, test_labels))\n",
    "# Just about the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.5402979e-20 4.5703349e-41 9.5402979e-20]\n",
      " [4.5703349e-41 1.7467022e-37 0.0000000e+00]\n",
      " [1.7467022e-37 0.0000000e+00 5.6051939e-45]\n",
      " ...\n",
      " [0.0000000e+00 9.8090893e-45 0.0000000e+00]\n",
      " [2.8025969e-45 0.0000000e+00 4.2038954e-45]\n",
      " [0.0000000e+00 2.8025969e-45 0.0000000e+00]]\n",
      "[[9. 9. 9.]\n",
      " [9. 9. 1.]\n",
      " [9. 9. 9.]\n",
      " ...\n",
      " [4. 3. 3.]\n",
      " [1. 1. 1.]\n",
      " [5. 5. 5.]]\n",
      "(10000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 9\n",
    "# Getting the training set for the blender model (XGBoost)\n",
    "x_val_predictions = np.empty((len(x_val), len(myClassifiers)), dtype=np.float32)\n",
    "print(x_val_predictions)\n",
    "for index, classifier in enumerate(myClassifiers):\n",
    "    x_val_predictions[:, index] = classifier.predict(x_val)\n",
    "print(x_val_predictions)\n",
    "print(x_val_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have to train the classifiers with the samples reshaped like this:\n",
    "# svc.predict(x_train[].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryuparish/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:15:17] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now to train the xgb blender model on the predictions\n",
    "import xgboost as xgb\n",
    "xgbc = xgb.XGBClassifier()\n",
    "xgbc.fit(x_val_predictions, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9712"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now to blend and see the scores on the test set and see the resulting score\n",
    "x_test_predictions = np.empty((len(test_data), len(myClassifiers)), dtype=np.float32)\n",
    "for index, classifier in enumerate(myClassifiers):\n",
    "    x_test_predictions[:, index] = classifier.predict(test_data)\n",
    "xgbc.score(x_test_predictions, test_labels)\n",
    "# It did just as good at the voting classifier earlier. Maybe a super small margin worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
